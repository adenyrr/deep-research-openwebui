### Deep Research At Home! 

**Version:** 0.5.0

Forked by adenyrr from : https://github.com/atineiatte/deep-research-at-home
For local only, use original script.

This script was designed to perform internet searches with searxng locally, but to use external tools for everything else.

The OCR engine, synthesis models, search models, and embedding models can come from different sources, as long as the API is OpenAI-compatible (format: https://domain.url/api/v1).

## WARNING

THIS SCRIPT HAS BEEN **MASSIVELY** MODIFIED USING AI TOOLS.

I use this script on a local openwebui instance, and I do not guarantee its use or safety in any way. I am simply sharing a script that I use and find useful.


Feel free to fork it! :)


## üîß Configuration des embeddings et streaming (exemples)

- Activer une API compatible OpenAI (ex. OpenRouter / Mistral) :

```python
p = Pipe()
p.valves.OPENAI_API_URL = "https://api.openrouter.ai/v1"
p.valves.OPENAI_API_KEY = "sk_..."
# Mod√®les selon votre fournisseur
p.valves.RESEARCH_MODEL = "mistral-large"
p.valves.EMBEDDING_MODEL = "text-embedding-3-large"
```

- Contr√¥ler le batching / async des embeddings :

```python
p.valves.EMBEDDING_BATCH_SIZE = 1       # 1 = pas de batching
p.valves.EMBEDDING_BATCH_ASYNC = True     # ex√©cute les batches concurrents
p.valves.EMBEDDING_BATCH_TIMEOUT = 30     # timeout par batch (s)
# D√©lai minimum (secondes) entre requ√™tes non-batch (single-item) ‚Äî utile si votre fournisseur impose un rate-limit strict
p.valves.EMBEDDING_NONBATCH_RATE_LIMIT = 1.0
```

- Configurer le chunking des textes longs pour embeddings :

```python
p.valves.EMBEDDING_CHUNK_SIZE = 1200      # taille max (caract√®res) par chunk
p.valves.EMBEDDING_CHUNK_OVERLAP = 200    # overlap (caract√®res) entre chunks
```

- Exemple de streaming de compl√©tion (OpenAI-compatible requis) :

```python
async def on_stream(partial):
    if partial is None:
        print("[STREAM DONE]")
        return
    print(partial, end="", flush=True)

await p.generate_completion(
    model=p.get_research_model(),
    messages=[{"role":"user","content":"Fais un r√©sum√© court de ceci."}],
    stream=True,
    stream_handler=on_stream
)
```

Cette configuration vous permet d'utiliser OpenRouter/Mistral pour les embeddings et la g√©n√©ration, de parall√©liser les batches d'embeddings et de g√©rer le streaming des r√©ponses en temps r√©el.

- Masquer les recherches pendant l'ex√©cution (stream final seulement) :

```python
# n'affiche que la synth√®se finale et les erreurs, masque les statuts et messages interm√©diaires
# Les messages interm√©diaires sont toujours envoy√©s mais marqu√©s `masked=True` et encapsul√©s
# dans des balises `<think>...</think>` pour que l'interface puisse les afficher sur demande.
p.valves.STREAM_FINAL_ONLY = True
```

- D√©tails d'affichage :
# Les √©v√©nements ont maintenant un champ `masked: True`. Les messages interm√©diaires sont envoy√©s en clair mais avec `masked=True` et `masked_stream=True`; par d√©faut l'interface peut les afficher envelopp√©s dans `<think>...</think>`. Si vous pr√©f√©rez, le serveur peut lui‚Äëm√™me encapsuler ces messages c√¥t√© serveur en activant `p.valves.STREAM_WRAP_MASKED = True`. Les erreurs (`level == "error"`) ne sont jamais masqu√©es.

- Utilisation d'Ollama : Ollama est facultatif. Si vous fournissez `OPENAI_API_URL` (et facultativement `OPENAI_API_KEY`), l'API OpenAI-compatible sera utilis√©e par d√©faut.

- Configuration avanc√©e par service : vous pouvez maintenant sp√©cifier des endpoints distincts pour les embeddings et la synth√®se :

```python
p.valves.EMBEDDING_API_URL = "https://api.openrouter.ai/v1"
p.valves.EMBEDDING_API_KEY = "sk_..."  # facultatif
p.valves.SYNTHESIS_API_URL = "https://api.openrouter.ai/v1"
p.valves.SYNTHESIS_API_KEY = "sk_..."  # facultatif
```

Si une option sp√©cifique n'est pas fournie, le `OPENAI_API_URL` / `OPENAI_API_KEY` servira de fallback.

- OCR Mistral pour PDF : vous pouvez activer l'OCR Mistral pour les PDFs scann√©s (mod√®le `mistral-ocr-latest` par d√©faut) :

```python
p.valves.MISTRAL_OCR_ENABLED = True
p.valves.MISTRAL_API_URL = "https://mistral.ai/api/v1"
p.valves.MISTRAL_API_KEY = "sk_mistral..."  # facultatif
p.valves.MISTRAL_OCR_MODEL = "mistral-ocr-latest"
```

La fonction testera d'abord les extractions textuelles locales (PyPDF2/pdfplumber). Si ces m√©thodes √©chouent et que `MISTRAL_OCR_ENABLED=True`, le PDF sera envoy√© au endpoint Mistral pour OCR. Notez que les d√©tails d'API peuvent varier selon les fournisseurs; le client essaie plusieurs patterns d'endpoint courants.
